{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 12)\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|PassengerId|Embarked|Pclass|                Name|   Sex| Age|SibSp|Parch|          Ticket|   Fare|Cabin|Survived|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "|          1|       S|     3|Braund, Mr. Owen ...|  male|22.0|    1|    0|       A/5 21171|   7.25| null|       0|\n",
      "|          2|       C|     1|Cumings, Mrs. Joh...|female|38.0|    1|    0|        PC 17599|71.2833|  C85|       1|\n",
      "|          3|       S|     3|Heikkinen, Miss. ...|female|26.0|    0|    0|STON/O2. 3101282|  7.925| null|       1|\n",
      "|          4|       S|     1|Futrelle, Mrs. Ja...|female|35.0|    1|    0|          113803|   53.1| C123|       1|\n",
      "|          5|       S|     3|Allen, Mr. Willia...|  male|35.0|    0|    0|          373450|   8.05| null|       0|\n",
      "|          6|       Q|     3|    Moran, Mr. James|  male|null|    0|    0|          330877| 8.4583| null|       0|\n",
      "|          7|       S|     1|McCarthy, Mr. Tim...|  male|54.0|    0|    0|           17463|51.8625|  E46|       0|\n",
      "|          8|       S|     3|Palsson, Master. ...|  male| 2.0|    3|    1|          349909| 21.075| null|       0|\n",
      "|          9|       S|     3|Johnson, Mrs. Osc...|female|27.0|    0|    2|          347742|11.1333| null|       1|\n",
      "|         10|       C|     2|Nasser, Mrs. Nich...|female|14.0|    1|    0|          237736|30.0708| null|       1|\n",
      "|         11|       S|     3|Sandstrom, Miss. ...|female| 4.0|    1|    1|         PP 9549|   16.7|   G6|       1|\n",
      "|         12|       S|     1|Bonnell, Miss. El...|female|58.0|    0|    0|          113783|  26.55| C103|       1|\n",
      "|         13|       S|     3|Saundercock, Mr. ...|  male|20.0|    0|    0|       A/5. 2151|   8.05| null|       0|\n",
      "|         14|       S|     3|Andersson, Mr. An...|  male|39.0|    1|    5|          347082| 31.275| null|       0|\n",
      "|         15|       S|     3|Vestrom, Miss. Hu...|female|14.0|    0|    0|          350406| 7.8542| null|       0|\n",
      "|         16|       S|     2|Hewlett, Mrs. (Ma...|female|55.0|    0|    0|          248706|   16.0| null|       1|\n",
      "|         17|       Q|     3|Rice, Master. Eugene|  male| 2.0|    4|    1|          382652| 29.125| null|       0|\n",
      "|         18|       S|     2|Williams, Mr. Cha...|  male|null|    0|    0|          244373|   13.0| null|       1|\n",
      "|         19|       S|     3|Vander Planke, Mr...|female|31.0|    1|    0|          345763|   18.0| null|       0|\n",
      "|         20|       C|     3|Masselmani, Mrs. ...|female|null|    0|    0|            2649|  7.225| null|       1|\n",
      "+-----------+--------+------+--------------------+------+----+-----+-----+----------------+-------+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql as sparksql\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as func\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "spark = SparkSession.builder.appName('titanic').getOrCreate()\n",
    "\n",
    "train = spark.read.csv('/home/ravisankar/Desktop/datasets/titanic/train.csv', inferSchema=True,header=True)\n",
    "train_pd=pd.read_csv('/home/ravisankar/Desktop/datasets/titanic/train.csv')\n",
    "train_shape=train_pd.shape\n",
    "print(train_shape)\n",
    "train.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: double (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: double (nullable = true)\n",
      " |-- Cabin: string (nullable = true)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----+\n",
      "|Survived|count|\n",
      "+--------+-----+\n",
      "|       1|  342|\n",
      "|       0|  549|\n",
      "+--------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train.groupBy('Survived').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.createOrReplaceTempView('titanic_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+-----------------+\n",
      "|   Sex|gender_count|          percent|\n",
      "+------+------------+-----------------+\n",
      "|female|         314|35.24130190796858|\n",
      "|  male|         577|64.75869809203142|\n",
      "+------+------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Sex ,count(Sex) as gender_count,count(sex)*100/sum(count(sex)) over() as percent from titanic_table GROUP BY Sex\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------------+\n",
      "|   Sex|gender_count|           percent|\n",
      "+------+------------+------------------+\n",
      "|female|         233| 68.12865497076024|\n",
      "|  male|         109|31.871345029239766|\n",
      "+------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Sex, count(Sex) AS gender_count, \\\n",
    "100*count(sex)/sum(count(sex)) over() AS percent \\\n",
    "FROM titanic_table \\\n",
    "WHERE Survived = 1 \\\n",
    "GROUP BY sex\").show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------+------------------+\n",
      "|   Sex|gender_count|           percent|\n",
      "+------+------------+------------------+\n",
      "|female|         233| 68.12865497076024|\n",
      "|  male|         109|31.871345029239766|\n",
      "+------+------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Sex, count(Sex) AS gender_count, \\\n",
    "100*count(sex)/sum(count(sex)) over() AS percent \\\n",
    "FROM titanic_table \\\n",
    "WHERE Survived = 1 \\\n",
    "GROUP BY sex\").show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----------+-----------------+\n",
      "|Pclass|class_count|          percent|\n",
      "+------+-----------+-----------------+\n",
      "|     1|        136|39.76608187134503|\n",
      "|     3|        119| 34.7953216374269|\n",
      "|     2|         87|25.43859649122807|\n",
      "+------+-----------+-----------------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEGCAYAAACevtWaAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvIxREBQAAErhJREFUeJzt3X+s3fV93/HnK9fmR2MCqn3XLLbDdYvR4oZAx41ZFKWzWEqMiC6VgrGdNZSJxgnUiaVsU822MMZClNFoFUP8gddRUAUyBqbhEK9oa0fakYT5XmrIjOPFIbS+NV1tIEQOIbbDe3/ci3d6ufiee32vL/74+ZAs3e/3fO73vo8vPP2933t+pKqQJLXlHbM9gCRp+hl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBs2ZrS+8YMGC6uvrm60vL0knpaGhoQNV1TvRulmLe19fH4ODg7P15SXppJTkL7pZ52UZSWqQcZekBhl3SWrQrF1zl9S+w4cPMzw8zGuvvTbbo5x0zjjjDBYtWsTcuXOn9PnGXdKMGR4e5qyzzqKvr48ksz3OSaOqePHFFxkeHmbJkiVTOoaXZSTNmNdee4358+cb9klKwvz584/rJx7jLmlGGfapOd6/N+MuSQ06Za659238+myPMKOe/8oVsz2CNKHp/v+w2//ub731Vu6//356enp4xzvewV133cUll1xyXF9769atPPvss2zcuPG4jgMwb948Dh48eNzH6XTKxF3Sqelb3/oWjz76KE899RSnn346Bw4c4NChQ1197pEjR5gzZ/xMDgwMMDAwMJ2jTisvy0hq2gsvvMCCBQs4/fTTAViwYAHvec976Ovr48CBAwAMDg6yYsUKAG6++WbWrVvHZZddxjXXXMMll1zCzp07jx5vxYoVDA0Ncc8997B+/XpeeeUV+vr6eP311wF49dVXWbx4MYcPH+b73/8+K1eu5OKLL+YjH/kI3/3udwH4wQ9+wIc+9CE++MEP8sUvfnFG7rdxl9S0yy67jL1793L++edzww038I1vfGPCzxkaGuKRRx7h/vvvZ82aNWzZsgUY+Ydi3759XHzxxUfXnn322Vx44YVHj/u1r32Nj33sY8ydO5d169Zxxx13MDQ0xFe/+lVuuOEGADZs2MD111/P9u3befe73z0D99q4S2rcvHnzGBoaYtOmTfT29rJ69WruueeeY37OwMAAZ555JgBXX301Dz74IABbtmxh1apVb1q/evVqHnjgAQA2b97M6tWrOXjwIN/85jdZtWoVF110EZ/5zGd44YUXAHjiiSdYu3YtAJ/61Kem667+LV5zl9S8np4eVqxYwYoVK7jgggu49957mTNnztFLKWMfT/7Od77z6McLFy5k/vz5PPPMMzzwwAPcddddbzr+wMAAN954Iy+99BJDQ0Nceuml/PjHP+acc85hx44d48400w8R9cxdUtN2797N9773vaPbO3bs4Nxzz6Wvr4+hoSEAHn744WMeY82aNdx222288sorXHDBBW+6fd68eSxfvpwNGzbw8Y9/nJ6eHt71rnexZMmSo2f9VcXTTz8NwIc//GE2b94MwH333Tct93Osrs7ck6wEbgd6gN+vqq+Ms+Zq4GaggKer6pPTOKekBszGQ3YPHjzI5z73OX74wx8yZ84czjvvPDZt2sSuXbu47rrr+PKXvzzhwyKvuuoqNmzYcMxffq5evZpVq1bx+OOPH9133333cf311/OlL32Jw4cPs2bNGi688EJuv/12PvnJT3L77bfziU98Yrru6t+Sqjr2gqQH+D/ArwHDwHZgbVU927FmKbAFuLSqXk7yd6rqb4513P7+/jqRb9bh49ylE2/Xrl28733vm+0xTlrj/f0lGaqq/ok+t5vLMsuBPVX1XFUdAjYDV45Z82ngzqp6GWCisEuSZlY3cV8I7O3YHh7d1+l84PwkTyT59uhlnDdJsi7JYJLB/fv3T21iSdKEuon7eL/SHXstZw6wFFgBrAV+P8k5b/qkqk1V1V9V/b29E76/q6QGTHTpV+M73r+3buI+DCzu2F4E7BtnzSNVdbiqfgDsZiT2kk5hZ5xxBi+++KKBn6Q3Xs/9jDPOmPIxunm0zHZgaZIlwF8Ba4Cxj4T5L4ycsd+TZAEjl2mem/JUkpqwaNEihoeH8TLs5L3xTkxTNWHcq+pIkvXAY4w8FPLuqtqZ5BZgsKq2jt52WZJngZ8B/7yqXpzyVJKaMHfu3Cm/k5COT1ePc6+qbcC2Mftu6vi4gC+M/pEkzTKfoSpJDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktSgruKeZGWS3Un2JNk4zu3XJtmfZMfon9+a/lElSd2aM9GCJD3AncCvAcPA9iRbq+rZMUsfqKr1MzCjJGmSujlzXw7sqarnquoQsBm4cmbHkiQdj27ivhDY27E9PLpvrE8keSbJQ0kWj3egJOuSDCYZ3L9//xTGlSR1o5u4Z5x9NWb7a0BfVX0A+O/AveMdqKo2VVV/VfX39vZOblJJUte6ifsw0HkmvgjY17mgql6sqp+Obv5H4OLpGU+SNBXdxH07sDTJkiSnAWuArZ0Lkvzdjs0BYNf0jShJmqwJHy1TVUeSrAceA3qAu6tqZ5JbgMGq2gp8PskAcAR4Cbh2BmeWJE1gwrgDVNU2YNuYfTd1fHwjcOP0jiZJmiqfoSpJDTLuktQg4y5JDerqmrs02/o2fn22R5gxz3/litkeQQ3yzF2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBxl2SGmTcJalBXcU9ycoku5PsSbLxGOuuSlJJ+qdvREnSZE0Y9yQ9wJ3A5cAyYG2SZeOsOwv4PPDkdA8pSZqcbs7clwN7quq5qjoEbAauHGfdvwVuA16bxvkkSVPQTdwXAns7todH9x2V5FeAxVX16LEOlGRdksEkg/v375/0sJKk7nQT94yzr47emLwD+D3gn050oKraVFX9VdXf29vb/ZSSpEnpJu7DwOKO7UXAvo7ts4D3A48neR74B8BWf6kqSbOnm7hvB5YmWZLkNGANsPWNG6vqlapaUFV9VdUHfBsYqKrBGZlYkjShCeNeVUeA9cBjwC5gS1XtTHJLkoGZHlCSNHlzullUVduAbWP23fQWa1cc/1iSpOPhM1QlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUHGXZIaZNwlqUFdxT3JyiS7k+xJsnGc2z+b5DtJdiT5n0mWTf+okqRuTRj3JD3AncDlwDJg7Tjxvr+qLqiqi4DbgH8/7ZNKkrrWzZn7cmBPVT1XVYeAzcCVnQuq6kcdm+8EavpGlCRN1pwu1iwE9nZsDwOXjF2U5LeBLwCnAZdOy3SSpCnp5sw94+x705l5Vd1ZVb8E/A7wr8Y9ULIuyWCSwf37909uUklS17qJ+zCwuGN7EbDvGOs3A78+3g1Vtamq+quqv7e3t/spJUmT0k3ctwNLkyxJchqwBtjauSDJ0o7NK4DvTd+IkqTJmvCae1UdSbIeeAzoAe6uqp1JbgEGq2orsD7JR4HDwMvAb87k0JKkY+vmF6pU1TZg25h9N3V8vGGa55IkHQefoSpJDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktQg4y5JDTLuktSgrl7yV5Kmqm/j12d7hBn1/FeumO0RxuWZuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1yLhLUoOMuyQ1qKu4J1mZZHeSPUk2jnP7F5I8m+SZJH+c5NzpH1WS1K0J456kB7gTuBxYBqxNsmzMsj8H+qvqA8BDwG3TPagkqXvdnLkvB/ZU1XNVdQjYDFzZuaCq/kdVvTq6+W1g0fSOKUmajG7ivhDY27E9PLrvrVwH/NfjGUqSdHy6eVXIjLOvxl2Y/AbQD/zDt7h9HbAO4L3vfW+XI0qSJqubM/dhYHHH9iJg39hFST4K/EtgoKp+Ot6BqmpTVfVXVX9vb+9U5pUkdaGbuG8HliZZkuQ0YA2wtXNBkl8B7mIk7H8z/WNKkiZjwrhX1RFgPfAYsAvYUlU7k9ySZGB02e8C84AHk+xIsvUtDidJOgG6eiemqtoGbBuz76aOjz86zXNJko6Dz1CVpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqkHGXpAYZd0lqUFdxT7Iyye4ke5JsHOf2X03yVJIjSa6a/jElSZMxYdyT9AB3ApcDy4C1SZaNWfaXwLXA/dM9oCRp8uZ0sWY5sKeqngNIshm4Enj2jQVV9fzoba/PwIySpEnq5rLMQmBvx/bw6D5J0ttUN3HPOPtqKl8sybokg0kG9+/fP5VDSJK60E3ch4HFHduLgH1T+WJVtamq+quqv7e3dyqHkCR1oZu4bweWJlmS5DRgDbB1ZseSJB2PCeNeVUeA9cBjwC5gS1XtTHJLkgGAJB9MMgysAu5KsnMmh5YkHVs3j5ahqrYB28bsu6nj4+2MXK6RJL0N+AxVSWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBnUV9yQrk+xOsifJxnFuPz3JA6O3P5mkb7oHlSR1b8K4J+kB7gQuB5YBa5MsG7PsOuDlqjoP+D3g3033oJKk7nVz5r4c2FNVz1XVIWAzcOWYNVcC945+/BDwj5Jk+saUJE3GnC7WLAT2dmwPA5e81ZqqOpLkFWA+cKBzUZJ1wLrRzYNJdk9l6JPEAsbc/5kUf1aaTn7vTm6tf//O7WZRN3Ef7wy8prCGqtoEbOria570kgxWVf9sz6HJ83t3cvP7N6KbyzLDwOKO7UXAvrdak2QOcDbw0nQMKEmavG7ivh1YmmRJktOANcDWMWu2Ar85+vFVwJ9U1ZvO3CVJJ8aEl2VGr6GvBx4DeoC7q2pnkluAwaraCvwn4A+T7GHkjH3NTA59kjglLj81yu/dyc3vHxBPsCWpPT5DVZIaZNwlqUHGXZIa1M3j3CXpbSvJcqCqavvoS6OsBL5bVdtmebRZ5S9UdcpL8vcYeZb1k1V1sGP/yqr6o9mbTBNJ8q8Zed2rOcB/Y+TZ848DHwUeq6pbZ2+62WXcZ1iSf1JVfzDbc2h8ST4P/DawC7gI2FBVj4ze9lRV/f3ZnE/HluQ7jHzfTgf+GlhUVT9KciYj/1h/YFYHnEVelpl5/wYw7m9fnwYurqqDoy9V/VCSvqq6nfFfVkNvL0eq6mfAq0m+X1U/AqiqnyR5fZZnm1XGfRokeeatbgJ+4UTOoknreeNSTFU9n2QFI4E/F+N+MjiU5Oeq6lXg4jd2JjkbMO46br8AfAx4ecz+AN888eNoEv46yUVVtQNg9Az+48DdwAWzO5q68KtV9VOAquqM+Vz+/0uinJKM+/R4FJj3RiA6JXn8xI+jSbgGONK5o6qOANckuWt2RlK33gj7OPsPcAJf9vftyF+oSlKDfBKTJDXIuEtSg4y7mpXkZ0l2JPnfSR5M8nPHWHtzkn92IueTZpJxV8t+UlUXVdX7gUPAZ2d7IOlEMe46VfwZcB5AkmuSPJPk6SR/OHZhkk8n2T56+8NvnPEnWTX6U8DTSf50dN8vJ/lfoz8hPJNk6Qm9V9Jb8NEyalaSg1U1b/R9fR8G/gj4U+A/Ax+uqgNJfr6qXkpyM3Cwqr6aZH5VvTh6jC8B/7eq7hh9qvvKqvqrJOdU1Q+T3AF8u6ruG30byp6q+sms3GGpg2fuatmZSXYAg8BfMvJ2kJcCD40+DpqqGu+N3N+f5M9GY/6PgV8e3f8EcE+STzPylpMA3wL+RZLfAc417Hq78ElMatlPquqizh1JAkz04+o9wK9X1dNJrgVWAFTVZ5NcAlwB7Bh9Zuv9SZ4c3fdYkt+qqj+Z5vshTZpn7jrV/DFwdZL5AEl+fpw1ZwEvJJnLyJk7o2t/qaqerKqbGHn24+Ikvwg8V1X/AdgKnLKvQqi3F8/cdUqpqp1JbgW+keRnwJ8D145Z9kXgSeAvgO8wEnuA3x39hWkY+UfiaWAj8BtJDjPykrO3zPidkLrgL1QlqUFelpGkBhl3SWqQcZekBhl3SWqQcZekBhl3SWqQcZekBv0/v/YcvGMuhigAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.sql(\"SELECT Pclass, count(Pclass) as class_count,\\\n",
    "         count(Pclass)*100/sum(count(Pclass)) over() AS percent \\\n",
    "         FROM titanic_table \\\n",
    "         WHERE Survived=1 GROUP BY Pclass\").show()\n",
    "import matplotlib.pyplot as plt\n",
    "sex_table=train_pd.pivot_table(index=\"Pclass\",values=\"Survived\")\n",
    "sex_table.plot.bar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+---------+\n",
      "| Age|age_count|\n",
      "+----+---------+\n",
      "|24.0|       15|\n",
      "|36.0|       11|\n",
      "|22.0|       11|\n",
      "|27.0|       11|\n",
      "|35.0|       11|\n",
      "|30.0|       10|\n",
      "|19.0|        9|\n",
      "|18.0|        9|\n",
      "|32.0|        9|\n",
      "|31.0|        8|\n",
      "|29.0|        8|\n",
      "| 4.0|        7|\n",
      "|28.0|        7|\n",
      "|17.0|        6|\n",
      "|34.0|        6|\n",
      "|25.0|        6|\n",
      "|33.0|        6|\n",
      "|48.0|        6|\n",
      "|40.0|        6|\n",
      "|42.0|        6|\n",
      "+----+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT Age, count(Age) as age_count FROM titanic_table WHERE Survived == 1 GROUP BY Age ORDER BY age_count DESC\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    714.000000\n",
       "mean      29.699118\n",
       "std       14.526497\n",
       "min        0.420000\n",
       "25%       20.125000\n",
       "50%       28.000000\n",
       "75%       38.000000\n",
       "max       80.000000\n",
       "Name: Age, dtype: float64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_pd['Age'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "176"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#train.filter((train['Survived']==1)&(train['Age']>='14')).count()\n",
    "train.filter((train['Survived']==1)&(train['Sex']=='female')&(train['Age']>=14)).count()\n",
    "#train.filter((train['Survived']==1)&(train['Age']>='14')).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Embarked|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|Fare|Cabin|Survived|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       S|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7|   00|       0|\n",
      "|          2|       C|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|  71|  C85|       1|\n",
      "|          3|       S|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|   8|   00|       1|\n",
      "|          4|       S|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|  53| C123|       1|\n",
      "|          5|       S|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8|   00|       0|\n",
      "|          6|       Q|     3|    Moran, Mr. James|  male| 30|    0|    0|          330877|   8|   00|       0|\n",
      "|          7|       S|     1|McCarthy, Mr. Tim...|  male| 54|    0|    0|           17463|  52|  E46|       0|\n",
      "|          8|       S|     3|Palsson, Master. ...|  male|  2|    3|    1|          349909|  21|   00|       0|\n",
      "|          9|       S|     3|Johnson, Mrs. Osc...|female| 27|    0|    2|          347742|  11|   00|       1|\n",
      "|         10|       C|     2|Nasser, Mrs. Nich...|female| 14|    1|    0|          237736|  30|   00|       1|\n",
      "|         11|       S|     3|Sandstrom, Miss. ...|female|  4|    1|    1|         PP 9549|  17|   G6|       1|\n",
      "|         12|       S|     1|Bonnell, Miss. El...|female| 58|    0|    0|          113783|  27| C103|       1|\n",
      "|         13|       S|     3|Saundercock, Mr. ...|  male| 20|    0|    0|       A/5. 2151|   8|   00|       0|\n",
      "|         14|       S|     3|Andersson, Mr. An...|  male| 39|    1|    5|          347082|  31|   00|       0|\n",
      "|         15|       S|     3|Vestrom, Miss. Hu...|female| 14|    0|    0|          350406|   8|   00|       0|\n",
      "|         16|       S|     2|Hewlett, Mrs. (Ma...|female| 55|    0|    0|          248706|  16|   00|       1|\n",
      "|         17|       Q|     3|Rice, Master. Eugene|  male|  2|    4|    1|          382652|  29|   00|       0|\n",
      "|         18|       S|     2|Williams, Mr. Cha...|  male| 30|    0|    0|          244373|  13|   00|       1|\n",
      "|         19|       S|     3|Vander Planke, Mr...|female| 31|    1|    0|          345763|  18|   00|       0|\n",
      "|         20|       C|     3|Masselmani, Mrs. ...|female| 30|    0|    0|            2649|   7|   00|       1|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# fill in missing values\n",
    "train_f = train.na.fill('00', subset=['Cabin'])\n",
    "#train_f=train.na.fill('null',subset=['Age'])\n",
    "\n",
    "#train_f.show()\n",
    "from pyspark.sql.functions import mean\n",
    "mean = train_f.select(mean(train['Age'])).collect()\n",
    "mean_age = mean[0][0]\n",
    "train_f = train_f.na.fill(mean_age,['Age'])\n",
    "#train_f = train.na.fill('00', subset=['Cabin'])\n",
    "train_f = train_f.withColumn(\"Age\", func.round(train_f[\"Age\"]).cast('integer'))\n",
    "train_f = train_f.withColumn(\"Fare\", func.round(train_f[\"Fare\"]).cast('integer'))\n",
    "train_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import (VectorAssembler,OneHotEncoder,\n",
    "                                StringIndexer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Embarked|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|Fare|Cabin|Survived|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       S|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7|   00|       0|\n",
      "|          2|       C|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|  71|  C85|       1|\n",
      "|          3|       S|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|   8|   00|       1|\n",
      "|          4|       S|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|  53| C123|       1|\n",
      "|          5|       S|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8|   00|       0|\n",
      "|          6|       Q|     3|    Moran, Mr. James|  male| 30|    0|    0|          330877|   8|   00|       0|\n",
      "|          7|       S|     1|McCarthy, Mr. Tim...|  male| 54|    0|    0|           17463|  52|  E46|       0|\n",
      "|          8|       S|     3|Palsson, Master. ...|  male|  2|    3|    1|          349909|  21|   00|       0|\n",
      "|          9|       S|     3|Johnson, Mrs. Osc...|female| 27|    0|    2|          347742|  11|   00|       1|\n",
      "|         10|       C|     2|Nasser, Mrs. Nich...|female| 14|    1|    0|          237736|  30|   00|       1|\n",
      "|         11|       S|     3|Sandstrom, Miss. ...|female|  4|    1|    1|         PP 9549|  17|   G6|       1|\n",
      "|         12|       S|     1|Bonnell, Miss. El...|female| 58|    0|    0|          113783|  27| C103|       1|\n",
      "|         13|       S|     3|Saundercock, Mr. ...|  male| 20|    0|    0|       A/5. 2151|   8|   00|       0|\n",
      "|         14|       S|     3|Andersson, Mr. An...|  male| 39|    1|    5|          347082|  31|   00|       0|\n",
      "|         15|       S|     3|Vestrom, Miss. Hu...|female| 14|    0|    0|          350406|   8|   00|       0|\n",
      "|         16|       S|     2|Hewlett, Mrs. (Ma...|female| 55|    0|    0|          248706|  16|   00|       1|\n",
      "|         17|       Q|     3|Rice, Master. Eugene|  male|  2|    4|    1|          382652|  29|   00|       0|\n",
      "|         18|       S|     2|Williams, Mr. Cha...|  male| 30|    0|    0|          244373|  13|   00|       1|\n",
      "|         19|       S|     3|Vander Planke, Mr...|female| 31|    1|    0|          345763|  18|   00|       0|\n",
      "|         20|       C|     3|Masselmani, Mrs. ...|female| 30|    0|    0|            2649|   7|   00|       1|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Sex_indexer=StringIndexer(inputCol='Sex',outputCol='SexIndex')\n",
    "Sex_encoder=OneHotEncoder(inputCol='SexIndex',outputCol='SexVector')\n",
    "\n",
    "Pclass_indexer=StringIndexer(inputCol='Pclass',outputCol='PclassIndex')\n",
    "Pclass_encoder=OneHotEncoder(inputCol='PclassIndex',outputCol='PclassVector')\n",
    "\n",
    "Age_indexer=StringIndexer(inputCol='Age',outputCol='AgeIndex')\n",
    "Age_encoder=OneHotEncoder(inputCol='AgeIndex',outputCol='AgeVector')\n",
    "\n",
    "\n",
    "\n",
    "Embarked_indexer=StringIndexer(inputCol='Embarked',outputCol='EmbarkedIndex')\n",
    "Embarked_encoder=OneHotEncoder(inputCol='EmbarkedIndex',outputCol='EmbarkedVector')\n",
    "\n",
    "Name_indexer=StringIndexer(inputCol='Name',outputCol='NameIndex')\n",
    "Name_encoder=OneHotEncoder(inputCol='NameIndex',outputCol='NameVector')\n",
    "\n",
    "Ticket_indexer=StringIndexer(inputCol='Ticket',outputCol='TicketIndex')\n",
    "Ticket_encoder=OneHotEncoder(inputCol='TicketIndex',outputCol='TicketVector')\n",
    "Fare_indexer=StringIndexer(inputCol='Fare',outputCol='FareIndex')\n",
    "Fare_encoder=OneHotEncoder(inputCol='FareIndex',outputCol='FareVector')\n",
    "Cabin_indexer=StringIndexer(inputCol='Cabin',outputCol='CabinIndex')\n",
    "Cabin_encoder=OneHotEncoder(inputCol='CabinIndex',outputCol='CabinVector')\n",
    "train_f.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler=VectorAssembler(inputCols=['PassengerId','EmbarkedVector','PclassVector','NameVector','SexVector','AgeVector','SibSp','Parch','TicketVector','FareVector','CabinVector'],outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "dtc = DecisionTreeClassifier(labelCol='Survived',featuresCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[Sex_indexer, Pclass_indexer, Age_indexer,Name_indexer,Embarked_indexer, Ticket_indexer,Fare_indexer, Cabin_indexer, Sex_encoder,\n",
    "                           Pclass_encoder, Age_encoder,Name_encoder,Embarked_encoder,Ticket_encoder,Fare_encoder,Cabin_encoder, assembler, dtc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "|PassengerId|Embarked|Pclass|                Name|   Sex|Age|SibSp|Parch|          Ticket|Fare|Cabin|Survived|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "|          1|       S|     3|Braund, Mr. Owen ...|  male| 22|    1|    0|       A/5 21171|   7|   00|       0|\n",
      "|          2|       C|     1|Cumings, Mrs. Joh...|female| 38|    1|    0|        PC 17599|  71|  C85|       1|\n",
      "|          3|       S|     3|Heikkinen, Miss. ...|female| 26|    0|    0|STON/O2. 3101282|   8|   00|       1|\n",
      "|          4|       S|     1|Futrelle, Mrs. Ja...|female| 35|    1|    0|          113803|  53| C123|       1|\n",
      "|          5|       S|     3|Allen, Mr. Willia...|  male| 35|    0|    0|          373450|   8|   00|       0|\n",
      "|          6|       Q|     3|    Moran, Mr. James|  male| 30|    0|    0|          330877|   8|   00|       0|\n",
      "|          7|       S|     1|McCarthy, Mr. Tim...|  male| 54|    0|    0|           17463|  52|  E46|       0|\n",
      "|          8|       S|     3|Palsson, Master. ...|  male|  2|    3|    1|          349909|  21|   00|       0|\n",
      "|          9|       S|     3|Johnson, Mrs. Osc...|female| 27|    0|    2|          347742|  11|   00|       1|\n",
      "|         10|       C|     2|Nasser, Mrs. Nich...|female| 14|    1|    0|          237736|  30|   00|       1|\n",
      "|         11|       S|     3|Sandstrom, Miss. ...|female|  4|    1|    1|         PP 9549|  17|   G6|       1|\n",
      "|         12|       S|     1|Bonnell, Miss. El...|female| 58|    0|    0|          113783|  27| C103|       1|\n",
      "|         13|       S|     3|Saundercock, Mr. ...|  male| 20|    0|    0|       A/5. 2151|   8|   00|       0|\n",
      "|         14|       S|     3|Andersson, Mr. An...|  male| 39|    1|    5|          347082|  31|   00|       0|\n",
      "|         15|       S|     3|Vestrom, Miss. Hu...|female| 14|    0|    0|          350406|   8|   00|       0|\n",
      "|         16|       S|     2|Hewlett, Mrs. (Ma...|female| 55|    0|    0|          248706|  16|   00|       1|\n",
      "|         17|       Q|     3|Rice, Master. Eugene|  male|  2|    4|    1|          382652|  29|   00|       0|\n",
      "|         18|       S|     2|Williams, Mr. Cha...|  male| 30|    0|    0|          244373|  13|   00|       1|\n",
      "|         19|       S|     3|Vander Planke, Mr...|female| 31|    1|    0|          345763|  18|   00|       0|\n",
      "|         20|       C|     3|Masselmani, Mrs. ...|female| 30|    0|    0|            2649|   7|   00|       1|\n",
      "+-----------+--------+------+--------------------+------+---+-----+-----+----------------+----+-----+--------+\n",
      "only showing top 20 rows\n",
      "\n",
      "root\n",
      " |-- PassengerId: integer (nullable = true)\n",
      " |-- Embarked: string (nullable = true)\n",
      " |-- Pclass: integer (nullable = true)\n",
      " |-- Name: string (nullable = true)\n",
      " |-- Sex: string (nullable = true)\n",
      " |-- Age: integer (nullable = true)\n",
      " |-- SibSp: integer (nullable = true)\n",
      " |-- Parch: integer (nullable = true)\n",
      " |-- Ticket: string (nullable = true)\n",
      " |-- Fare: integer (nullable = true)\n",
      " |-- Cabin: string (nullable = false)\n",
      " |-- Survived: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_data,test_data = train_f.randomSplit([0.7,0.3])#spliting train.csv into 70% for training and 30% for testing\n",
    "train_f.show()\n",
    "train_f.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o4817.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 318.0 failed 1 times, most recent failure: Lost task 0.0 in stage 318.0 (TID 2690, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:118)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier$$anonfun$train$1.apply(DecisionTreeClassifier.scala:121)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier$$anonfun$train$1.apply(DecisionTreeClassifier.scala:101)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:101)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-113-14a2443582b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#fitting the training model into the pipeline <3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdtc_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m#predicting the data B-)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    107\u001b[0m                     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# must be an Estimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    110\u001b[0m                     \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mindexOfLastEstimator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, dataset, params)\u001b[0m\n\u001b[1;32m    130\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 132\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    133\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m             raise ValueError(\"Params must be either a param map or a list/tuple of param maps, \"\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m         \u001b[0mjava_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjava_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_copyValues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/ml/wrapper.py\u001b[0m in \u001b[0;36m_fit_java\u001b[0;34m(self, dataset)\u001b[0m\n\u001b[1;32m    290\u001b[0m         \"\"\"\n\u001b[1;32m    291\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transfer_params_to_java\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 292\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_java_obj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    293\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_fit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     61\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    326\u001b[0m                 raise Py4JJavaError(\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 raise Py4JError(\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o4817.fit.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 318.0 failed 1 times, most recent failure: Lost task 0.0 in stage 318.0 (TID 2690, localhost, executor driver): org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n\nDriver stacktrace:\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1887)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1875)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1874)\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1874)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:926)\n\tat scala.Option.foreach(Option.scala:257)\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:926)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:2108)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2057)\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2046)\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:737)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2061)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2082)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2126)\n\tat org.apache.spark.rdd.RDD.count(RDD.scala:1168)\n\tat org.apache.spark.ml.tree.impl.DecisionTreeMetadata$.buildMetadata(DecisionTreeMetadata.scala:118)\n\tat org.apache.spark.ml.tree.impl.RandomForest$.run(RandomForest.scala:106)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier$$anonfun$train$1.apply(DecisionTreeClassifier.scala:121)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier$$anonfun$train$1.apply(DecisionTreeClassifier.scala:101)\n\tat org.apache.spark.ml.util.Instrumentation$$anonfun$11.apply(Instrumentation.scala:183)\n\tat scala.util.Try$.apply(Try.scala:192)\n\tat org.apache.spark.ml.util.Instrumentation$.instrumented(Instrumentation.scala:183)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:101)\n\tat org.apache.spark.ml.classification.DecisionTreeClassifier.train(DecisionTreeClassifier.scala:46)\n\tat org.apache.spark.ml.Predictor.fit(Predictor.scala:118)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: org.apache.spark.SparkException: Failed to execute user defined function($anonfun$9: (string) => double)\n\tat org.apache.spark.sql.catalyst.expressions.GeneratedClass$GeneratedIteratorForCodegenStage1.processNext(Unknown Source)\n\tat org.apache.spark.sql.execution.BufferedRowIterator.hasNext(BufferedRowIterator.java:43)\n\tat org.apache.spark.sql.execution.WholeStageCodegenExec$$anonfun$11$$anon$1.hasNext(WholeStageCodegenExec.scala:619)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat scala.collection.Iterator$$anon$11.hasNext(Iterator.scala:409)\n\tat org.apache.spark.util.Utils$.getIteratorSize(Utils.scala:1817)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.rdd.RDD$$anonfun$count$1.apply(RDD.scala:1168)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.SparkContext$$anonfun$runJob$5.apply(SparkContext.scala:2101)\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:90)\n\tat org.apache.spark.scheduler.Task.run(Task.scala:121)\n\tat org.apache.spark.executor.Executor$TaskRunner$$anonfun$10.apply(Executor.scala:402)\n\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:1360)\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:408)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\t... 1 more\nCaused by: org.apache.spark.SparkException: StringIndexer encountered NULL value. To handle or skip NULLS, try setting StringIndexer.handleInvalid.\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:251)\n\tat org.apache.spark.ml.feature.StringIndexerModel$$anonfun$9.apply(StringIndexer.scala:246)\n\t... 19 more\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_data)#fitting the training model into the pipeline <3 \n",
    "dtc_predictions = model.transform(test_data)#predicting the data B-)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dtc_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-111-f57d704eff27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc_evaluator\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassClassificationEvaluator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabelCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Survived\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictionCol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prediction\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetricName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"accuracy\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mdtc_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0macc_evaluator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc_predictions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'A Decision Tree algorithm had an accuracy of: {0:2.2f}%'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtc_acc\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dtc_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# Select (prediction, true label) and compute test error\n",
    "acc_evaluator = MulticlassClassificationEvaluator(labelCol=\"Survived\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "dtc_acc = acc_evaluator.evaluate(dtc_predictions)\n",
    "\n",
    "print('A Decision Tree algorithm had an accuracy of: {0:2.2f}%'.format(dtc_acc*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
